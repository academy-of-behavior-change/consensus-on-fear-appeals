---
output:
  html_document:
    code_folding: show
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: true
      smooth_scroll: false
  pdf_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
require(userfriendlyscience);
safeRequire('here');
knitr::opts_chunk$set(echo=TRUE);
knitr::opts_chunk$set(comment=NA);

rejoinderDOI <- "INSERT_REJOINDER_DOI_HERE";

figCountr <- function(nr = getOption("fearAppealFigureCounter", 1)) {
  options(fearAppealFigureCounter = nr+1);
  if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "latex") {
    return("");
  } else {
    return(paste("Figure", nr, ": "));
  };
}
```

# Towards consensus on fear appeals: supplementary file

This supplementary appendix contains more detailed discussion of the commentaries on the provocative article [Ignoring theory and misinterpreting evidence: the false belief in fear appeals](https://doi.org/10.1080/17437199.2017.1415767). To enable a constructive, collaborative focus in the rejoinder, as well as do justice to the thoughtful comments without exceeding the 1500 word boundary for commentaries and rejoinders, we decided to include the point-by-point responses in this supplementary file.

This version was generated at `r format(Sys.time(), '%H:%M:%S on %Y-%m-%d %Z (GMT%z)');`. Earlier versions can be viewed at the [history section for this file](https://github.com/academy-of-behavior-change/consensus-on-fear-appeals/commits/master/consensus_on_fear_appeals_-_rejoinder_to__false_belief_in_fear_appeals__debate.html) at this project's [GitHub repository](https://github.com/academy-of-behavior-change/consensus-on-fear-appeals).

Note that because this text is an integral part of the commentary, if you wish to cite this text, please cite the commentary:

<div style="margin-left: 2em; text-indent: -2em;">

Peters, G.-J. Y., Ruiter, R. A. C., ten Hoor, G. A., Kessels, L. E., Kok, G. (2018) Towards Consensus on Fear Appeals: A rejoinder to the commentaries on Kok, Peters, Kessels, ten Hoor & Ruiter (2018). *Health Psychology Review, 12*. doi: [`r rejoinderDOI`](https://doi.org/`r rejoinderDOI`)

</div>

This supplementary file is hosted at the [Open Science Framework](https://osf.io/a2v46/).

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# Recapping "The false belief in fear appeals"

The argument that threatening communication is almost always in the best case a sub-optimal solution, and is therefore a most inconvenient default 'go to behavior change method', was based on three premises:

1. The determinant targeted by threatening communication is risk perception. Many determinants exist, and among those, risk perception is rarely the best predictor of behavior.
2. Many risk behaviors are hard to change, so target populations will usually have low self-efficacy.
3. The psychological evidence and theory on how humans process threatening information predicts that unless efficacy is high, threatening communication has no or a counterproductive effect.

So, we argued as follows:

1. Threatening communication targets a determinant that plays only a minor role compared to other determinants such as self-efficacy or perceived norm, which caps its maximum possible effect at a relatively low level (since effects attenuate through the causal chain, see e.g. Webb & Sheeran, 2006);
2. Because threatening communication is usually applied in populations low in self-efficacy (because behavior change interventions are usually employed when problems are encountered in trying to promote behavior change), achieving the modest effects they might achieve if most effective requires that they are accompanied by an intervention that successfully increases self-efficacy;
3. Such interventions are, in practice, often hard to combine with threatening communication (e.g. tobacco packaging does not provide enough room for health communications that can target both perceived threat and self-efficacy sufficiently strongly);
4. Given that dozens of other determinants exist, many of which enable larger effects as they more strongly predict behavior, and given that dozens of other methods of behavior change exist, most of which run a lower risk of backfiring, it seems wise to replace threatening communication in almost all instances with a more effective health communication.

In addition to this core argument, we also analysed why threatening communication as a method is so popular.

As we mentioned in our paper, the belief in fear appeals is intuitive and persistent in lay people. So far, we did not manage to suppress this 'naïve theory' with scientific arguments and we obviously have not convinced some of our colleagues that fear appeals (with a strong focus on threat) are relatively ineffective. However, our observation that naïve theories are not easily replaced by scientific evidence is a phenomenon also seen in earlier studies (e.g., Shtulman & Valcarcel, 2012). In our own studies, evidence and theory were not enough to counter the intuitive ideas about fear appeals in students (ten Hoor et al., 2012), nor in intervention developers, policymakers, politicians, scientists, and advertising professionals (Peters et al., 2017). Additionally Peters et al. (2014) have shown that those who are further removed from intervention development (e.g. funders) were more in favor of fear appeals (Peters, Ruiter, Verboon & Kok, 2017).

However, this is never used as an argument in discourse about threatening communication. Instead, other arguments are used:

1. Observational studies conducted in tandem with policy changes including the introduction of warning labels suggest that the warning labels have an effect.
2. Experimental designs (RTCs) are so hard to implement when evaluating health promotion campaigns that deriving casual conclusions from observational designs is justified.
3. Experimental designs (RCTs) have insufficient external validity, and threatening communication relies in aspects of reality that cannot be captured in controlled designs, so deriving casual conclusions from observational designs is justified.
4. Studies measuring proxies of behavior, such as intention or attitude, show effects of threatening communication.

We aimed to pre-emptively counter each of these arguments as follows:

1. It is indeed challenging to conduct experiments that have both high internal and high external validity when studying the effects of threatening communications in large-scale health promotion campaigns (such as warning labels). However, this degree of difficulty does not elevate observational evidence to the level where it affords causal conclusions.
2. It is not *impossible* to conduct experiments that have both high internal validity and high external validity. For example, while randomizing participants to a condition where they have to smoke is unethical, such 'show stopping' obstacles don't exist in this case. The fact that something is expensive, hard or inconvenient should not mean we therefore refrain from doing it, or lower our evidential standards.
3. If lower-quality evidence (e.g. observational studies) is not in agreement with higher-quality evidence (fundamental studies designed to establish how psychological processes work), the lower-quality evidence should not prevail, not even if there is a lot of that lower-quality evidence (the lower-quality evidence can, after all, be obtained through applied research, which is easier to obtain funding for).
4. In the case of threatening communication, self-reports of reasoned behavioral determinants such as attitude and  intention have low validity.

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# Two preliminary points and studies to resolve them

Academic debates should ideally culminate in a set of studies that can then be preregistered and conducted and replicated to answer the questions that remain unanswered. After all, disagreement will often only be possible because no appropriate empirical evidence exists to determine which of the competing hypotheses is most plausible.

Therefore, we will try to identify these points of disagreement and aim to conduct studies to resolve them in a constructive manner. We have already started on a literature review, some results of which we will present below.

## The importance of risk perception for behavior

As we argued in the original paper, risk perception is a relatively unimportant predictor of behavior compared to other determinants. Our argument was mainly based on theories that explain health behavior. We will here supplement this argument with some empirical data. We have started a systematic literature synthesis to summarize the literature on determinants of smoking initiation and successful smoking cessation.

Although still very much a work in progress (see the [Open Science Framework repository](https://osf.io/vs39h/)), we have processed enough papers to give some initial impression of which determinants have been studied, and what the relative importance of these determinants is.

So far, we extracted the most data for studies into smoking initiation. When meta-analysing the associations of determinants to smoking initiation (for behavior outcomes only), the following picture emerges (literally):

![`r figCountr();`The association of various determinants to smoking initiation ](https://github.com/academy-of-behavior-change/smoking-synthesis/raw/master/results-working-path/20180311-initiationResults-light-400px.png)

Less studies investigated successful smoking cessation, and in fact, those studies only measured three determinants. Here, the following picture emerges:

![`r figCountr();`The association of various determinants to smoking cessation ](https://github.com/academy-of-behavior-change/smoking-synthesis/raw/master/results-working-path/20180311-cessationResults-light-400px.png)

Overall, the conclusion is that targeting risk perception does not seem an obvious choice. On the basis of these results, one would choose to target subjective and descriptive norms to prevent smoking initiation, and self-efficacy to promote succesful cessation.

Now, as we said, this project is still very much a work in progress. Other determinants may emerge as most important for initiation and successful cessation, and these results should not yet be used to base policies or intervention on.

However, they *should* clearly show that risk perception is just one of many determinants that can be targeted; and by no means an obvious 'default choice'. Targeting risk perception (or any determinant) requires justification - and the justification should be based on a comparison with other potential intervention targets (i.e. other determinants), not on the simple fact that the association of risk perception to behavior is not completely absent.

This systematic literature synthesis is one of three studies we believe can help achieve consensus, at least regarding whether threatening communication is the best choice to employ as health communication on tobacco packaging. The Open Science Framework project page for this project is at [https://osf.io/vs39h/](https://osf.io/vs39h/).

## The validity of self-reported intention as operationalisation of intention

In our original paper, we have not been sufficiently clear about why measures of intention are problematic in the context of interventions employing threatening communication. We understand from the commentaries that our problem with intention was sometimes assumed to be based on intention being a weak predictor in the case of behaviors commonly targeted using threatening communication. This is not the case: we did not mean that intention as a psychological construct has less predictive value.

We meant that threatening communications invalidate *self-report measures* of intention. Specifically, operationalisations of the intention construct that rely on self-reports have lower validity if participants are engaging in fear control. These fear control processes are initiated in response to threatening communication if individuals believe they cannot effectively deter a threat (Witte, 1992).

The goal of fear control processes is to decrease the experienced fear. This is a functional process that facilitates effective functioning (van 't Riet & Ruiter, 2013). Research into fear control processes is plentiful and has resulted in four main categories of fear control processes being distinguished: avoidance, denial, suppression, and cognitive reappraisal. Cognitive reappraisal decreases the experienced fear by engaging other beliefs: "someone who receives threatening information can also use a defence strategy in which the threat is neither avoided nor denied, but additional beliefs are adopted so that the threat is encompassed in a cognitive 'business as usual' framework" (van 't Riet & Ruiter, 2013, p. 23). Cognitive reappraisal appears to be the most effective form of fear control (van 't Riet & Ruiter, 2013).

Reporting a high intention to change the risk behavior represents a form of cognitive reappraisal.

For example, imagine a campaign to promote healthy sleeping patterns. A threatening communication might target the perceived severity of a lack of adequate sleep by presenting graphic depictions of short- and long-term consequences such as deteriorated judgment, mood, and learning ability, and increased risks of obesity, cardiovascular disease, and mortality.

Imagine being a target individual who does not consistently get enough sleep.  Exposure to the health communication will trigger threat appraisal, and let us assume that the outcome is both a high perceived severity and a high perceived susceptibility. Therefore, the target individual perceived a threat and will engage in coping appraisal. If the individual can identify one or more actions that effectively diminish the threat and the individual is confident they can perform at least one of those actions, they will commence danger control, and their motivation to engage in protective action will increase.

However, imagine that the target individual is not confident that they can adopt healthy sleeping patterns. In that case, the individual cannot engage in danger control, and therefore, engages in fear control. Defensive reactions are dynamic entities, interrelated and changing over time: it is not the case that one of the four processes is selected and the others then remain inactive.

For an individual with fear control motivation, a questionnaire asking about intentions to engage in protective action in the future offers an excellent way to engage in cognitive reappraisal. In our example individual, their train of thought might run as follows:

> "I did not know that my sleeping habits expose me to all those risks. That is bad, and threatens me. Is there something I can do? Not really, I've tried so many times already... This is just also a particularly busy period of course: things will get better once this one thing resolves. Hmm, a question asking whether I want to adopt healthy sleeping patterns in the future. Of course I do! I'll just start working on going to bed a bit earlier."

The use of questions measuring intention as a device for cognitive reappraisal severely undermines the validity of these questions as an operationalisation of the intention construct. If individuals use intention questions as cognitive reappraisal aids, there are no changes in their determinants or in their actual intention. In their working memory, they perceive their intention to be high, and report it as such; but this high intention does not reflect the state of their cognitions (or beliefs), but instead reflects a convenient way to diminish the experienced fear.

This is very clear when adopting a pragmatic nihilism perspective (Peters & Crutzen, 2017), where intention can be defined as *consisting* of the underlying determinants, instead of as being a modular, separate psychological entity that is influenced by those determinants (other modular, separate psychological entities). None of the constituent determinants or sub-determinants actually changes: the individual does not suddenly know better strategies to get to bed in time; the individual is not persuaded that their important social referents value healthy sleeping patterns; the individual is not aware of which environmental cues to alter to promote habit change; et cetera.

Therefore, the intention construct, defined as a structure of cognitive and affective associations, does not change, and remains at a lower level than reported. The reported intention, being higher, is not a valid operationalisation of this theoretical intention construct.

The inconsistency in results between effects of threatening communication on intention and on behavior (see our response to Brewer, Hall & Noar's commentary) offers some weak support for this hypothesis, but hardly enough to base intervention development or policy on. And we know of no direct empirical investigations of this hypothesis.

Therefore, we will design a study to test this hypothesis that reporting a high intention can be utilised as a fear control strategy. The Open Science Framework project page for this project is at [https://osf.io/m8k5j/](https://osf.io/m8k5j/).

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# Discussing the commentaries

The commentaries respond both to our arguments why threatening communication is almost always best foregone in favour of other methods of behavior change that target other determinants of behavior, and to our pre-empting of counter-arguments against those primary arguments. We will refer back to these points as we discuss each commentary below, in alphabetical order.

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# Borland (2018): Misinterpreting theory and ignoring evidence: fear appeals can actually work

Borland's (2018) commentary is a good example of why there is less disagreement than one might think at first glance. However, this commentrary also suggests that we did not manage to convey our exact position sufficiently clearly, for which we hereby would like to apologise. We will now correct that error.

## Fear appeals *can*, indeed, work

Most of Borland's (2018) commentary is well summarized by the commentary title: "fear appeals can actually work".

We agree that fear appeals can work. Actually, when the method is applied as prescribed (see e.g. Kok et al., 2016), and so includes a component that effectively increases self-efficacy in the target individual, we'd even agree that fear appeals will work as a rule. If the method is not used correctly, for example when severity is targeted but susceptibility and efficacy are not, we'd say that as a rule, fear appeals will *not* work (unless the target individual already has high susceptibility and high efficacy).

In a situation where fear appeals have no effect, of course, in some samples an effect will be found nonetheless. When an alpha of 5% is used, that effect will be significant in one in twenty studies. In the absence of consistent and comprehensive preregistration, the garden of forking paths (Gelman & Loken, 2014), or, if one is cynically inclined, researcher degrees of freedom (Simmons, Nelson & Simonsohn, 2011; 2012), can interact with publication bias to spawn a literature where occcasionally, evidence of fear appeal effectiveness can be found.

However, as we said, we do not believe all reports of effective fear appeal interventions are statistical artifacts. We believe that when well applied, fear appeals can have an effect and successfully increase risk perception, which may ultimately translate into an effect on behavior.

## In situations where fear appeals work, they're relatively impotent

However, our point was not that fear appeals can never have an effect. Our point was that even though fear appeals might work, they are usually an unwise choice. There are two reasons for this.

First, proper use of fear appeals requires also increasing efficacy. Whereas an intervention targeting efficacy, if used in isolation, can be successsful even if it achieves relatively small effects, an efficacy intervention that is used to enable a threat-based behavior change intervention must have a substantial effect. After all, it needs to increase target individuals' efficacy to sufficiently high levels to prevent defensive reactions.

Second, if one succeeds in coupling such a potent efficacy intervention with a successful threat intervention, one can indeed expect the fear appeal to increase risk perception. However, even then, risk perception is not usually one of the strongest predictors of behavior. Quite the opposite; risk perception is a relatively weak predictor of behavior (as we argued above in [The importance of risk perception for behavior]).

Therefore, when efficacy is high or successfully increased, so if a fear appeal is optimally successful, the weak association of the targeted determinant, risk perception, with behavior attenuates that effect considerably.

Now, we disagree on what happens in situations where efficacy is low. But, as we discuss in the main text of the rejoinder, we all agree that threat manipulations have in the best case scenario lower effectiveness in low efficacy situations. Given the relatively small role risk perception plays in behavior, in this best case scenario, any effect remaining of threat communications is then attenuated.

Thus, using fear appeals requires quite an effort (i.e. increasing efficacy to moderate-to-high levels) to achieve the maximum possible effect on risk perception, and even after all that effort, because risk perception is a comparatively weak determinant of behavior, only a weak effect can be expected on behavior.

If all that effort were to be invested in targeting other determinants, such as self-efficacy, response efficacy, attitude, or perceived norms (to name but a few potential intervention targets), one can expect a much stronger effect on behavior. There are two reasons for this, which mirror the reasons why fear appeals are an unwise choice.

First, most behavior change methods do not have conditions for effectiveness (Peters & Kok, 2016; Kok et al., 2016) that are quite so hard to meet, and for most behavior change methods, there are no debates where it is discussed whether failing tot satisfy those conditions can result in backfiring. Thus, correctly applying other methods for behavior change is easier and so less likely to fail, and the potential costs for failure are lower (i.e. no risk at occasional backfiring).

Second, other methods can target other determinants. since risk perception is, compared to other determinants such as self-efficacy, a relatively weak predictor of behavior, targeting other determinants has more potential for achieving behavior change.

## Fear appeals are not an important factor in behavior change

Our argument is that to realise the optimal effect on behavior, fear appeals are a particularly unwise choice. The optimal intervention targets the most important determinant(s) using the methods that have the strongest effect on those determinant(s). The mere fact that stronger predictors of behavior exist in our view makes threatening communication hard to justify. Sure, it can have an effect. But at-risk population deserve more than 'an effect': they deserve the strongest effect we can realise.

Because of these reasons, we also disagree with Borland's (2018) claim that:

> Kok et al. manage to avoid the findings of their own meta-analysis (Peters, Ruiter, & Kok, 2013) showing cases where fear appeals can work, especially when self-efficacy is high. It was common enough in these cases to find significant effects, thus they should have concluded that they can be common enough to be important factors in behaviour change.

As we argued, finding some positive effects does not justify concluding that risk perception or threat communications can be an *important* factor. It is enough to conclude that it can be a factor; but comparison to the effects of other methods, targeting other determinants, is required to draw conclusions about importance.

## Reviews of 'population-level impacts'

Borland (2018) also argues:

> Extensive reviews of the population-level impacts of health communications, especially for smoking, clearly shows that graphic imagery including graphic personal stories, are the most effective way of communicating the risks of smoking and stimulating quitting activity (Durkin, Brennan, & Wakefield, 2012; National Cancer Institute, 2008; Wakefield, Liken, & Hornik, 2010). This evidence is ignored.

We have looked for evidence as to the relative effectiveness of threatening communication in these reviews, but found none. There does not seem to have been a comparison of different methods of behavior change, or behavior change techniques (BCTs; see, for example, the milestone BCT taxonomy paper by Abraham & Michie, 2008, and the more recent Michie et al., 2013 paper, as well as the Intervention Mapping taxonomy paper, Kok et al., 2016).

Now, it is true that the feasibility of learning about relative effectiveness of behavior change techniques through meta-analyses of applied evaluations of intervention campaigns has been strongly critized (Ogden, 2016; Peters & Kok, 2016; Peters, de Bruin & Crutzen, 2015). Still, any meta-analysis seeking to answer the question of which method is most effective should at least compare meta-analysed effect size estimates, or engage in some other systematic form of comparison, as well as explore publication bias, before conclusions can be drawn.

We identified one meta-analysis that did compare the effectiveness of different methods of behavior change (Bartlett, Sheeran & Hawley, 2014). It sought to compare the effectiveness of 47 behavior change techniques (or BCTs; BCTs are a conflation of methods for behavior change and their applications). First, this is a good illustration of narrow scope of the present debate: out of the 47 methods for behavior change in use, the present debate only concerns itself with the question of whether one of these may have an effect. Second, the four techniques identified as most effective were *Facilitate action planning/develop treatment plan*, *Prompt self-recording*, *Advise on methods of weight control*, and *Advise on/facilitate use of social support*, as well as *Linking COPD and smoking*. However, *Provide information on the health consequences of smoking and smoking cessation* was not associated to a difference in effect size.

Now, as we said, using this meta-analytic approach to learn about BCT effectiveness has substantial problems (Peters, de Bruin & Crutzen, 2015): what is needed is meta-analyses of experiments specifically testing each BCT's effect under different conditions. But, in this case, because the question is not whether threatening communication can have an effect but whether it is the most potent health communication that we can use on tobacco packaging, this meta-analysis by Bartlett, Sheeran & Hawley (2014) is a useful reminder of the number of BCTs at our disposal.

Thus, we did not ignore these publications Borland (2018) cited. We could just not find any evidence in them that supports any point other than 'threatening communication can have an effect'. As explained above, finding evidence of an effect of threatening communication does not mean that threatening communication is a good, let alone the best, approach to behavior change.

## Reactance dissipating

Borland (2018) cites one study to support his argument that perhaps, defensive reactions dissipate while some impact of threatening communication remains. However, this study did not compare threatening to non-threatening communications.

Instead, the study compared high intensity language messages (such as "Skin cancer is a grotesque growth of skin cells.", "Treatment of skin cancer involves cutting or burning tumors from the skin.", and "Tragically, about 7,200 Americans will die from melanoma, a very serious type of skin cancer, this year alone.") with low intensity language messages (such as "Skin cancer is an unusual growth of skin cells.", "Treatment of skin cancer involves removing tumors from the skin." and "Sadly, about 7,200 Americans will die from melanoma, a very serious type of skin cancer, this year alone."). It also compared messages formatted in inductive versus deductive logical styles (see Buller, Borland & Burgoon, 1998).

So, first, it is unlikely that these manipulations are valid operationalisations of threat (i.e. it is unlikely that the messages have different effects on perceived threat; and the study reports no measures that allow us to verify this).

Second, the study's authors stated "There were also some trends, but as the sample size is large in this group, they are not considered." (Buller, Borland & Burgoon, 1998). It appears odd to then ignore that directive and consider a trend as evidence nonetheless (considering trends as evidence is odd in any situation, but especially when the study's authors recommend against it).

Thus, this study does not appear to support the possiblity of dissipating reactance.

Of course, that does not mean that reactance does not dissipate. Absence of evidence, after all, is not evidence of absence.

We know of no systematic investigations of such dissipating reactance effects. It seems easy enough to study, so we would welcome preregistrations to explore this, especially when they apply full disclosure (Peters, Abraham & Crutzen, 2012) to enable swift or even parallel replications. Until such time as the results are in, however, it seems premature to assume that reactance dissipates yet effects on risk perception remain.

To us, this hypothesis of dissipating reactance seems too unlikely to warrant empirical investigation. Therefore, we invite Borland (or others who adhere to the hypothesis of dissipating reactance) to take the lead. We will gladly think along, of course, and may even assist by conducting a replication once the study is preregistered.

Note, however, that even if such effects on risk perception *would* remain, we refer back to our points made earlier: this still would not mean that fear appeals, or threatening communication, are a wise choice when one truly desires to be of most help to at-risk populations. Other methods of behavior change, targeting other determinants of behavior, can have stronger effects.

## The complex dynamics of behavior change

Borland (2018) discusses two more points that relate to the complexities of behavior change.

First, the fact that target individuals are exposed to behavior change interventions in a real world, filled with other people, organisations, buildings, and other environmental conditions and dynamics, is introduced as aspects of observed reality that we supposedly ignore. However, no theories or even evidence of how these dynamics moderate the effects of behavior change interventions (such as threatening communications on tobacco packaging) are offered.

Now, we do realise the important of environmental conditions for behavior change. These have a crucial role in the Intervention Mapping protocol that forms the framework for much of our health promotion intervention planning. However, the fact that interventions are administered in a complex environmental context is not magic. Potential moderation by contextual variables and processes creates a responsibility to study those hypothesized moderators using rigorous methods, so that these dynamics may be leveraged when crafting behavior change interventions. The potential presence of moderators does not justify letting observational studies prevail over more rigorous evidence (also see [Internal versus external validity]).

We do concur, however, that our failed attempts to influence broader policy is our failure. In our frequent contact with practitioners and policymakers alike, we often struggle with the phenomenon that our message, which boils down to things being considerably more complicated than people assume, is markedly less popular than competing messages that instead promote simplicity (such as reliance on one out of the dozens of behavior change methods, which conveniently also assuages the onus to first conduct thorough determinant studies).

Second, Borland (2018) claims that we fundamentally misunderstand the nature of complex behavior change. This is an curious accusation. We are more used to being accused of complicating things, for example when we argue that meta-analyses of applied studies suffer serious methodological limitations precluding strong conclusions as to the effectiveness of behavior change methods (Peters, de Buirn & Crutzen, 2015), that psychological constructs such as risk perception and attitude do not exist as modular entities in human psychology (Peters & Crutzen, 2017), that behavior change methods represent accelerated learning and are based on evolutionary learning processes (Crutzen & Peters, 2018), and of course that effective behavior change is too complicated to allow focusing on one behavior change method, instead requiring that first the determinants of a behavior are mapped (see e.g. Bartholomew, Parcel & Kok, 1998, and Bartholomew Eldredge, Markham, Ruiter, Fernández, Kok & Parcel).

However, the accusation represents more than a refreshing change of pace. It is also a bit worrying. It portrays our plea to stop treating threatening communication as the only or most optimal approach to behavior change as simplistic. This strongly suggests that our plea was insufficiently clear, given that it primarily comprises an imperative to acknowledge the complexity of behavior change.

In order to correct this misunderstanding, in the main rejoinder text, we attempt to apply some of the steps required to acknowledge this complexity to the example of tobacco packaging health communication, ans we illustrate those steps visually. We hope that will help to bring our point across that things are more, not less, complicated once the insistence on fear appeal use is abandoned. We hope that the next accusation, then, will again reassuringly concern our urge that things are more, rather than less, complicated.

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# Brewer, Hall & Noar: Pictorial cigarette pack warnings increase quitting

## The importance of smoking as a health behavior

Brewer, Hall & Noar (2018) start by reiterating how unhealthy smoking is, arguing that:

> actions that can reduce cigarette smoking, by preventing initiation or helping people quit, are critically important.

It is not clear why they open with this point. Of course, we are all in agreement. It may bear explicitation that their emphasis on this point may perhaps be misconstrued as implying that our argument (that fear appeals are generally to be avoided) is in some way based on the position that preventing smoking initiation or promoting cessation is insufficiently important. In fact, the opposite is true: exactly because we so strongly endorse the importance of preventing smoking initiation and promoting cessation, we wrote this paper. We try to promote replacing warning labels with health communications that are *more* effective at preventing smoking initiation and promoting cessation (see also Figure 1 in the [main text of the rejoinder](https://doi.org/`r rejoinderDOI`)).

## The Brewer et al. (2016) RCT

Brewer, Hall and Noar (2018) then cite a randomized controlled trial where behavior was an outcome measure as "convincing evidence of the behavioral effects of pictorial cigarette pack warnings". Before going into this evidence, let us test the design of this study against the basic criteria outlined in the paper to which Brewer, Hall, and Noar's (2018) commentary was a response:

> In summary, there are three criteria for correctly applying an experimental design to study the effectiveness of fear appeals: (1) at least two different interventions or manipulations that differ only with respect to the independent variables of interest, (2) random assignment of these treatments to participants and (in the case of fear appeals) (3) behaviour as an outcome measure.

The last of these is clearly satisfied, which in itself is commendable given the lamentable lacuna in the literature when it comes to studies into threatening communications with behavioral outcomes. The second one, this being an RCT, is also satisfied. However, the first criterion is not: there was no comparison to a non-threatening health communication (or to a control condition, for that matter). This means that this study cannot provide evidence of the effectiveness of threatening communication; it cannot exclude the possibility that *fewer* people make quit attempts when exposed to threatening communication.

However, for the sake of the discussion, let's assume that the comparison condition of this study (text-only warning labels) has no effect on behavior. That means that the effect sizes reported in this study would be an indication of the effect of the pictorial warning labels.

This study's primary outcome measure was a quit attempt (24 hours without smoking) during the 4 weeks of the study (reported at either 1, 2, 3 or 4 weeks). The study also had nine secondary outcome measures, two of which were behavioral. While the primary outcome was in fact not the desired behavior but a proxy (only a proportion of quit attempts proves successful), one of the two secondary behavioral outcomes was in fact exactly the desired outcome: *successful* quitting, operationalised as no smoking in the last week of the study (one can argue whether not smoking for one week is sufficiently valid as measurement of success, but given the paucity of studies with behavioral measures, this is already a great step forward). Where the first secondary measure was great, the second was a proxy even further removed from the desired behavior: foregone cigarettes.

## Effect sizes {.tabset}

### Text

The 95% confidence interval for the measure's association to type of threatening communication (i.e. textual vs. pictorial) was [1.09; 1.54] (this is an odds ratio; the conventional qualitative labeling starts at 'small' for odds ratios over 1.5; Rosenthal, 1996; Chen, Cohen & Chen, 2010). The point estimate for this odds ratio was 1.29.

The 95% confidence interval for the association between type of threatening communication (i.e. textual vs. pictorial) and likelihood of successfully quitting was [1.02; 2.29]. The point estimate for this odds ratio was 1.53.

Brewer et al. (2016) do not report a confidence interval for the association between type of threatening communication and forgoing cigarettes. When reconstructing the 95% confidence interval for Cohen's d, an interval of [-0.014; 0.155] was found.

In other words, while this study did obtain *significant* effects on behavior, this significance appeared mostly a consequence of the huge sample size. The confidence intervals for these effects showed that they are likely what could be qualified as small at best.

Now, one could, and Brewer et al. (2018) do, argue that a tiny effect can still "have a substantial benefit across the population of US smok-
ers" (2016; p. 909). If a tiny effect materialises in thousands of people, the those tiny effects can together still yield considerable healthcare savings, for example.

However, this is an invalid argument. Not because it is false; but because it doesn't ameliorate the small effects that were obtained. A small effect is still undesirable when larger effect can be obtained using a different method targeting a different determinant. If no alternatives were available, and not using threatening communication would mean no health communications could be placed on tobacco packaging, then this might be a valid point: but that is not the scenario.

These results are consistent with our argument that at best, threatening communication is a sub-optimal strategy for behavior change. The design, or more accurately, the operationalisation of the independent variable that Brewer et al. (2016) chose for this RCT precludes it from providing evidence the effectiveness of threatening communication; but if we take the charitable position that we assume that text-only warnings have zero effect (instead of being counter-productive), then this RCT can be taken to provide evidence that pictorial warning labels have trivial effects on behavior.

This leaves a lot of room for improvement. Replacing pictorial warning labels with health communications targeting determinants that are more relevant than risk perception, using methods for behavior change that are more effective than threatening communication, is a promising example of such improvement.

### Underlying analyses

```{r brewer-effect-size}

### Number from Table 3
forGoingCigs_means <- c(2.5, 2.7);
forGoingCigs_sds <- c(2.8, 2.9);
forGoingCigs_ns <- c(1077, 1070);
forGoingCigs_N <- sum(forGoingCigs_ns);

### Standard error for difference between means
(forgoingCigs_se <- sqrt((forGoingCigs_sds[1] / forGoingCigs_ns[1]) +
                         (forGoingCigs_sds[2] / forGoingCigs_ns[2])));

### t value
(forgoingCigs_t <- diff(forGoingCigs_means) / forgoingCigs_se);

### P value
2*(1-pt(forgoingCigs_t, forGoingCigs_N-2))

### Cohens' d, method 1
(forgoingCigarettesD <-
  convert.t.to.d(forgoingCigs_t, n1=forGoingCigs_ns[1], n2=forGoingCigs_ns[2]));

### Cohen's d, method 2
(forgoingCigarettesD <-
  convert.means.to.d(means=forGoingCigs_means,
                     sds=forGoingCigs_sds,
                     ns=forGoingCigs_ns));

### Cohen's d and confidence interval
confIntD(forgoingCigarettesD,
         n=forGoingCigs_N);

### P value based on Cohen's d
pdExtreme(forgoingCigarettesD,
          n=forGoingCigs_N);

### Confidence intervals around the means
meanConfInt(mean=forGoingCigs_means[1],
            sd=forGoingCigs_sds[1],
            n=forGoingCigs_ns[1]);
meanConfInt(mean=forGoingCigs_means[2],
            sd=forGoingCigs_sds[2],
            n=forGoingCigs_ns[2]);

### Confidence interval for difference between means
diff(forGoingCigs_means) + c(-1.96, 1.96) * forgoingCigs_se;

```

## Mediators (determinants) targeted by warning labels

Brewer, Hall and Noar (2018) then continue listing a number of potential mediators of potential effects of graphic warning labels. Studying mediators is notoriously hard to do, and evidence of mediation can only be obtained by designs that are both experimental and longitudinal (Roe, 2012; Spencer, Zanna & Fong, 2005), and even then is not straightforward (Green, Ha & Bullock, 2010). Unfortunately, the 2018 manuscript they cite has not yet been posted on a preprint server, so we cannot know whether this study has a design that enables it to provide evidence of this mediation.

However, regardless, it is necessarily the case that any mediator of an effect of an intervention is a determinant of behavior (literally by definition). Thus, Brewer, Hall and Noar (2018) implicitly argue that negative affect is a determinant of quitting, or, conversely, that negative affect is negatively associated to smoking. Given that smoking is used to cope with negative emotions, this does not seem straightforward. After all, are we to believe that warning labels permanently induce negative affect in smokers which then in turn facilitates quitting?

The theoretical mechanism at play here remains vague. This may be due in part because of the problems in studying mediators: without manipulating both the predictor and the hypothesized mediator, it is easy to incorrectly conclude that mediator occurs. Mediation, after all, requires that the mediator causally influences the criterion.

For example, imagine a hypothetical scenario where a threatening communication has an effect on three psychological constructs: perceived severity, disgust, and negative affect. Let us assume that we happen to conduct this study in a population high in efficacy and aware of their susceptibility to the threat, and therefore, their perceived threat increases and they engage in danger control, resulting in the desired behavior change.

If in this scenario, the three constructs (severity, disgust, and affect) are measured before and after exposure to the manipulation, mediation analyses cannot be informative as to whether any of these three constructs mediates the effect of the manipulation. In this hypothesis example, we *know* that severity is the mediator; but in a mediation analysis, disgust and negative affect would also emerge as 'significant' mediators. This is the case even though in this hypothetical example, they have no causal association with behavior and therefore, can by definition not be mediators.

Therefore, it is impossible to draw conclusions about whether a variable is a mediator based on a dataset collected with a design where the predictor was manipulated, and the potential mediator and the criterion were measured.

As we said, we do not know the exact specifics of the as yet unpublished manuscript cited by Brewer, Hall and Noar (2018): and so we cannot conclude that the manuscript can or cannot offer evidence as to potential mediation. But regardless of the quality of the evidence, any potential mediator is necessarily a determinant of behavior: and therefore, those mediators would have to be strong predictors of behavior for them to justify reliance on threatening communication (see [The importance of risk perception for behavior]). We know of no studies where disgust or negative affect emerged as relatively important determinants of behavior.

## Weak versus strong hypotheses

Finally, Brewer, Hall and Noar constructed a tabel based on data from two meta-analyses into threatening communication. They constructed this table to compare evidence for a 'strong hypothesis' (when efficacy is low, threatening communication can backfire) and a 'weak hypothesis' (when efficacy is low, threatening communication has a weaker effect):

| Source           | k   | Outcome    | With statement d | Without statement d | Efficacy hypothesis supported |
|------------------|-----|------------|------------------|---------------------|-------------------------------|
| Tannenbaum, 2015 | 110 | Attitudes  | .39*             | .14*                | Weak hypothesis               |
| Tannenbaum, 2015 | 161 | Intentions | .40*             | .27*                | Weak hypothesis               |
| Tannenbaum, 2015 | 70  | Behavior   | .43*             | 0.14                | Weak hypothesis               |
| Peters et al., 2013 | 7   | Behavior   | .40*             | -0.14               | Weak hypothesis               |
| Peters et al., 2013 | 6   | Behavior   | .31*             | -0.31               | Weak hypothesis               |

:Table 1: Effect sizes for threatening communications in two meta-analyses.

On the basis of this table, they conclude:

> These findings are reassuring because they demonstrate that fear appeals without efficacy statements do not backfire.

However, this table cannot demonstrate that fear appeals without efficacy statements do not backfire. It does show something else, however.

We are not sure why the first rows are included; particularly in this table, which does provide insight into effects on behavior, attitude and intention can safely (or rather: prudently) be omitted. These rows show that effects on attitude and intention are markedly smaller under low efficacy conditions, but while this in itself may be taken to imply that further attentuation nullifies the effects completely, the table in itself offers no evidence of this.

This table cannot demonstrate that fear appeals without efficacy do not backfire because the comparison between the weak and the strong hypothesis concerns an interaction effect, Specifically, the relevant theory (the Extended Parallel Process Model; Witte, 1992) predicts moderation: efficacy moderates the effect of threat. Testing that moderation, or the interaction as which it manifests as expressed by the contrast between Brewer, Hall and Noar's (2018) weak and the strong hypotheses, requires comparing the effect of a threatening communication under low efficacy conditions to the effect of a threatening communication under high efficacy conditions.

This information is not present in this table. First, the Tannenbaum meta-analysis did not systematically compare different levels of threat (but see the commentary by White & Albarracin, 2018), and so cannot answer any questions related to effects of threatening communication. Second, it did not consider efficacy (either through manipulation or by establishing efficacy levels in the study's samples). Therefore, we simply don't know whether those data concern the effects of threat manipulations under low efficacy.

So, although these data cannot demonstrate anything about the importance of efficacy, they do seem to imply that threat manipulation is not effective at changing behavior. At best, the effects are weak; which is exactly Brewer, Hall and Noar's (2018) weak hypothesis.

If either the weak or the strong hypothesis is accurate, threatening communication should be used as a last resort, instead of first weapon of choice. This in itself should be enough reason to abandon threat-based behavior change methods in favour of alternatives, of which there are many. Why keep using a teaspoon to dig a hole, when you're standing in front of a rack of shovels?

Interestingly, the fact that the weak and strong hypotheses emerge as the two most plausible competing models of reality can in itself be viewed as some form of evidence. If either is true, that in itself already implies that threatening communications are better avoided in favour of other methods, the effectiveness of which does not depend on a condition that is rarely true (e.g. smokers being confident that they can successfully cease smoking).

## When all you have is a hammer: Brewer, Hall and Noar's conclusion

In their conclusion, Brewer, Hall and Noar (2018) argue:

> All of this theorizing is interesting and important, and we behavioral scientists will spend the next decades refining hypotheses about how to improve fear communications.

This statement suggests a very strong commitment to fear communications as method of behavior change. As applied researchers, such a fixation on one specific method out of the hundreds that exist is somewhat alien to us, but of course we acknowledge that scientific progress requires vast numbers of fundamental studies into tightly constrained topics. At the same time, it is our strong conviction that accepting such constraints to one single approach is fatal in the application of science.

In this case, for example, fear communications are not the only or the best way to influence behavior. As we explained in the original paper, an overwhelming number of methods of behavior change exists, and these methods enable targeting very many determinants (some would argue an infinite number; Peters & Crutzen, 2017).

Perhaps Brewer, Hall and Noar (2018) operate within this severe constraint, and in the context of tobacco packaging, see health communication as synonymous to warning labels. However, a priori instating such constraints to one method, and as a necessary consequence, the psychological determinants that that method happens to be able to influence, sells smokers and vulnerable non-smokers short.

The severity of the health consequences of smoking entails a strong responsibility to behavior change researchers, practitioners, and policymakers to use the most powerful tools at our disposal to facilitate cessation and discourage initiation. This responsibility precludes attacking every problem with a hammer simply because we happen to hold it in our hands. We should take a step back, put away the hammer, and start inspecting this problem, so we may discover that it is, in fact, a bolt instead of a nail.

So far, the discussion, unfortunately, has centered on whether it is, perhaps, in some circumstances, possible to drive a bolt into a hole using a hammer. This discussion is not productive and inadvertedly contributes to maintenance of sub-optimal use of space on tobacco packaging for health communications. The question is not whether a hammer is of completely no use, might backfire, or can sometimes perhaps be used to drive a bolt into the wall. The question is what the optimal tool is to drive a bolt into the wall, so that we may employ it.

Actually, to take this metaphor one step further, thorough analysis of this problem might reveal that driving the bolt into the hole is not even the most promising approach to achieve our ultimate goal. For example, imagine that we have detected a round hole in a drain, and we happen to have a bolt handy that fits the hole perfectly. In this case, we could drive the bolt in the hole to fix the leak. If we would use a hammer, we would probably manage; but hammering the bolt in would enlarge the hole and so the bolt is likely to be dislodged soon. A wrench would be a better choice: it wouldn't enlarge the hole (but use the thread) and so the likelihood of bolt detachment would decrease. However, instead of fixating on using this bolt we happen to have handy, we could also use plumber's epoxy. Being designed exactly for fixing holes in drains, epoxy will be much more effective.

So, while, yes, risk perception (the bolt) is a determinant of behavior, and, yes, it is true that is has *some* effect in the case of smoking (the hole), it is almost never the most *important* determinant of behavior. Also in the case of smoking, there are determinants that have more effect, such as self-efficacy (plumber's epoxy).

So far, this discussion has focused on whether a hammer can work to drive a bolt in a leaky drain. We propose to take a step back and survey all tools at our disposal so as to find not a solution that may have some effect, but the solution that has the most effect.

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# Malouff: What Constitutes Evidence that Fear Appeals Have Positive Effects on Health Behavior?

## The conditions under which non-RCT evidence can help show causation

Malouff's (2018) thoughtful commentary starts by addressing our relative disdain for non-experimental designs *for studying causality*, but bases his argument on methodology and philosophy of science. This is refreshing after a number of arguments where the extreme complexity and costs of experimental research with warning labels was used as an argument to draw causal conclusions based on inadequate designs.

Now, first, we agree with the basic thesis that non-experimental evidence can contribute to understanding about causality. Malouff (2018) lists two examples that in our view serve well to explain why in this case, resorting to non-experimental evidence is not justified: smoking causes lung cancer and HIV causes AIDS. As Malouff (2018) puts it,

> Experimental methods are not feasible to study such matters precisely because such methods would require randomly assigning individuals to smoke for decades. That would be both unethical and unfeasible.

Given this unethical and unfeasible nature of the designs required to study the causal association between smoking and lung cancer and between HIV and AIDS, instead, vast amounts of resources have been invested in thousands of studies in various disciplines that eventually allowed us to draw causal conclusions.

The situation is very different for threatening communication.

First, it is not unethical of unfeasible to study the effects of threatening communication using experimental designs. Exposing smokers to both threatening communication and another, nonthreatening health communication to compare the effects on behavior is ethically acceptable and very feasible.

Second, the vast multidisciplinary evidence base available that in its aggregate provides convincing evidence that smoking can cause lung cancer and that HIV can cause AIDS does not exist in the case of threatening communication. While in the case of smoking and lung cancer as well as HIV and AIDS, the results from observational studies and theoretical models converge, such clear converge is absent from the fear appeal literature. That is, either side in this debate will argue that there is actually covergence, but there is no consensus as to where these results converge.

Fortunately, in this case, it *is* possible to conduct experiments with behavioral outcomes. There is no reason to confine ourselves to fumbling around with observational data: it is both ethical and feasible to conduct experiments.

## Internal versus external validity

Malouff (2018) also remarks on the external validity of experiments:

> Experiments that have ideal internal validity due to the use of an experimental research method may have weak or no external validity and thus limit generalizability to the general population. No one study is likely to have both ideal internal validity and ideal external validity.

We strongly agree especially with his latter statement (see also [Effects of campaign diffusion and effects]). We also agree with the first, but would add that degree of external validity is orthogonal to whether a study is observational or experimental. Indeed, experiments may have low external validity; so may observational studies. Experiments may also have low internal validity (for example, we would argue that fear appeal experiments operationalising intention using a questionnaire do, see [The validity of self-reported intention as operationalisation of intention]); so may observational studies.

However, experiments do not *as a rule* have low external validity. We agree with Malouff's (2018) observation that many psychology experiments in the published literature sampled from psychology students, and would add that this is also true for many observational studies in the published psychology literature. This is, however, not a characteristic of the experimental method, and a weakness that one can remedy when one designs an experiment to test the effectiveness of a behavior change intervention. In fact, to avoid counterproductive effects, it is generally advisable to pretest any intervention (Whittingham, Ruiter, Bolier, Lemmers, van Hasselt & Kok, 2009). Had this been done for warning labels, perhaps we would not have had this debate in the first place.

## Introspection as evidence

Malouff (2018) argues that one's own impressions of why might work for behavior change are indicative for what works. This is an example of the intuitive nature of threatening communication. However, unfortunately, introspection is no reliable method to obtain information about human psychology (e.g. Wilson & Dunn, 2004; Wilson & Bar-Anan, 2008). This is not to say that observation as a method has no value: it just cannot be relied on in introspection. We need systematic approaches to gathering data, as argues before, preferably using experimental designs if we are interested in causality.

## Smokers' low self-efficacy

Malouff's (2018) second point addresses our argument that smokers have low self-efficacy. Specifically, Malouff (2018) argues:

> Smokers who quit permanently typically make many attempts before they succeed (Chaiton et al., 2016). These statistics suggest that many smokers have self-efficacy about being able to quit.

In contrast, in our original paper, we argued:

> [...], self-efficacy will most probably be very low (most smokers have unsuccessfully tried quitting a number of times, see Condiotte & Lichtenstein, 1981; John, Meyer, Hapke, Rumpf, &  Schumann, 2004; Smit, Hoving, Schelleman-Offermans, West, & De Vries, 2014; Zhou et al., 2009).

We see now that we were insufficiently clear. Self-efficacy is a very important determinant for many behaviors, but there are very few behavioral determinants that represent both necessary and sufficient conditions for behavior change. Behavior and behavior change are the result of a complex interplay of many factors. Therefore, behavior change is possible under low efficacy: it's just unlikely.

In other words, smokers who manage to quit after repeated failures to do so, quit *despite low self-efficacy*, not because of high self-efficacy. One cannot derive high self-efficacy from failure. One of the sources of self-efficacy as postulated in Social Cognitive Theory is obtaining mastery experiences; and conversely, repeatedly experiencing failure decreases self-efficacy. This makes sense: otherwise people would never accept that they lack certain skills, and continue trying forever (Ntoumanis, Healy, Sedikides, Smith & Duda, 2014; Ntoumanis & Sedikides, 2018).

## Prevention is important

Here we are completely on the same page. Prevention is crucial and often cost-effective (Masters, Anwar, Collins, Cookson & Capewell, 2017). However, prevention is behavior change, and therefore, prevention should follow the best practices for behavior change as codified in dedicated prevention protocols such as, for example the Intervention Mapping protocol (Bartholomew et al., 2016).

This entails an imperative to not start thinking about behavior change from one's own experiences and beliefs, nor from those of the target population: but instead, to combine psychological theory, empirical evidence, and target population involvement to develop the optimal interventions.

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# Niederdeppe & Kemp: Ignoring Theory and Evidence

Nierdeppe and Kemp respond by arguing that our conclusion, that threatening communication is almost always better replaced by more effective methods for behavior change, ignored three bodies of evidence. Specifically, they argue that we ignore theories of campaign diffusion and effects; evidence from large-scale population studies; and theory on other negative emotions than fear as behavioral determinants.

## Effects of campaign diffusion and effects

Niederdeppe & Kemp (2018) argue that:

> In many cases, randomized trials may not offer the highest evidentiary standard for achieving this goal.

Unfortunately, they do not specify under which conditions the safeguards against confounding and bias as provided by randomization and  blinding are superseded by other virtues. They do explain that communication is a complex and diffuse social process. We completely agree, and would add that this complexity is a severe underestimate of the complexity of behavior change. Communication is, after all, only one aspect of successful behavior change, and the other aspects are no simpler (e.g. conducting needs assessments, mapping the determinants of behavior; using psychological theory and evidence to select methods for behavior change; involving target population members, implementers, stakeholders, and other key individuals while at the same time adequately confining their involvement, et cetera; see Bartholomew Eldredge et al., 2016).

We also agree that this complexity does not lend itself well to tightly controlled randomized designs. Indeed, studying this complex matter is in itself an even more complex endeavour. We are not clear why this should mean that the logical and methodological realities that render non-experimental designs so ill-suited for causal inference are irrelevant.

External validity is as important as internal validity, but not more important. With low internal validy, patterns observed in the sample cannot be assumed to be indicative for real-world dynamics, for example because associations that seem causal may be confounded.

Studies of real-world applications of behavior change interventions do not conveniently also provide the data required to gain understanding of the intricacies of behavior change methods and the conditions under which they work. Gaining an understanding of when exactly we should use which behavior change method requires an iterative protocol for evidence base accumulation (IPEBA; Peters, de Bruin & Crutzen, 2015).

We do not argue that studies with less control have no value; instead, we argue that like studies with more control, they have their place in the arsenal of a behavior change scientist. No single study can achieve both optimal internal and optimal external validity (see also [Internal versus external validity]). When seeking to answer questions relating to the specific mechanics of behavior change (or more broadly, psychology), one requires tightly controlled studies; when seeking to answer questions relating to the effectivenss of full-blown behavior change interventions (ideally, systematically designed, based on knowledge of the specific mechanics of behavior change), one requires evaluations in more 'messy' real-word settings.

Niederdeppe & Kemp also argue that:

> Both theory and evidence from population-based studies make clear that communication interventions can change behavior, but only tend to work when they (a) generate substantial levels of message exposure among the target audience, (b) diffuse through social networks, and (c) activate institutional changes that support the targeted behavior (e.g., Hornik, 2002; Wakefield, Loken, & Hornik, 2010).

While we agree with this first of these three statements, we are not convinced of (b) and (c). Only a few methods for behavior change leverage social networks, and only a few utilize institutional changes. In fact, proper application of many effective methods for behavior change (e.g. guided practice, modeling, tailoring, agenda setting) are not compatible with diffusing through social networks. While environmental changes can usefully support or even enable behavior change, positing that these are crucial to behavior change in general seems at odds with much health psychology theory and evidence. For example, Crutzen and Peters (2018) argue that all behavior change, even that occurring through the environment, must necessarily entail a change in the individual; Kwasnicka, Dombrowski, White and Sniehotta (2016) explain how environmental changes can relieve self-regulatory requirements in behavior maintenace; and McEachan, Conner, Taylor and Lawton (2011) show that up to one quarter of the variance in future behavior can successfully be predicted using only the determinants from one theory, all of which psychological.

Perhaps we misunderstood, and Niederdeppe & Kemp (2018) meant to argue that specifically for threatening communication, subsequent discussion of the stimulus, as well as realisation of environmental changes, are parameters for effectiveness of the behavior change method (see e.g. Kok, 2014; and Peters & Kok, 2016). Although the Extended Parallel Process Model lists a number of such parameters of effectivenss (e.g. the intervention must either successfully increase efficacy, or target a population already high in efficacy), it does not mention that discussion of the threatening stimulus is required. In fact, in its discussion of the processing of the threat, dialogue with other individuals does not play a role at all. We know of no theory positing that processing of threatening stimuli first requires dialogue or environmental change; nor do we know of any evidence where threat processing was compared for indivduals who did versus did not discuss the threat, or individuals who remained in the same environment versus individuals in adapted environments.

## Individuals versus population levels

Niederdeppe and Kemp continue to point out the flaws in the literature on fear appeal effectiveness. We could not agree more: this remains a tragically understudied topic, especially given the popularity of this approach.

They then continue to cite a number of literature syntheses of non-randomized studies, arguing:

> In isolation, none of these non-randomized population studies can fully rule out all possible confounders, as campaigns and warning labels are often implemented alongside other interventions. However, potential confounders differ across study and intervention contexts. In the aggregate, the sheer volume of evidence from these studies would seem difficult to ignore altogether, particularly relative to the limitations of the evidence offered by Kok et al. (2018).

We do not mean to ignore this literature. This literature indicates that, for example, textual and visual stimuli have different effects on attitude, intention, and behavior. Our point is not that threatening communication can never, under any circumstance, have an effect on behavior.

Our point is that at best, the effects that one can expect from threatening communication is inferior to the effects one can expect from health communication that is designed to target the most important determinants of behavior, using the theoretically most effective method.

## Warning labels as a method to target other determinants

Niederdeppe and Kemp (2018) also make the point made by Brewer, Hall and Noar (2018) that perhaps, threatening communication targets other determinants than fear or threat. We therefore refer the reader back to sections [Mediators (determinants) targeted by warning labels] and [The importance of risk perception for behavior].

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# Peters & Shoots-Reinhard: Don’t throw the baby out with the bath water

Peters and Shoots-Reinhard (2018) argue that the baby should not be thrown out with the bath water. We agree, but add that although this particular baby should not be thrown away, it should be avoided whenever possible, only to be used with great reluctance.

Peters' and Shoots-Reinhard's (2018) argument is based on their statement that 

> a strong base of converging evidence supports effectiveness of pictorial warning labels, with similar results shown for improving knowledge and encouraging smoking cessation (through either quit intentions or behavior change) in controlled experiments including recent clinical trials and large international observational studies (e.g., Brewer et al., 2016; Evans et al., 2015; Noar et al., 2016a, 2016b; Romer et al., 2018).

We have discussed these reviews and the evidence they can provide by virtue of their designs in previous sections (e.g. [Reviews of 'population-level impacts'], [The Brewer et al. (2016) RCT], and [The conditions under which non-RCT evidence can help show causation]). In short: we disagree about whether these studies provide evidence.

## Using behavioral intentions to study the effect of threatening communication

Peters and Shoots-Reinhard (2018) then argue that intention is a good predictor of behavior. We mostly agree (but would like to add an important nuance that is relevant regarding smoking).

Before going into the predictive value of intention (and therefore, the usefulness of measuring intention instead of behavior when studying behavior change interventions), let us refer back to section [The validity of self-reported intention as operationalisation of intention], where we explained why it is unwise to trust the validity of self-reports as operationalisations of intention when studying threat.

Peters and Shoots-Reinhard (2018) contest that reporting high intentions is a fear control process. They discount two studies we cited that showed that threatening communications yielded higher self-reported intentions, yet no effects on behavior. It is true that these studies are underpowered. At the same time, otherwise they are methodologically adequate, and they represent the sum total of evidence in this respect. To our knowledge, there exist no studies where a fear manipulation yielded increases in intention (or, for that matter, attitude or risk perception) that *did* translate to behavior change. On the other hand, there is the body of evidence on which the original formulation of the Extended Parallel Process Model was based (e.g. Kleinot & Rogers, 1982; Rogers & Mewborn, 1976).

Peters and Shoots-Reinhard (2018) then point out that it is useful to explore the conditions under which intention predicts behavior, in other words, the intention-behavior gap (Sheeran & Webb, 2016). We agree that looking at moderators of this association is useful. 

Thus, while it may be true that both studies we cited represent Type-1 errors, that question begs resolution through research.

As it stand, there seems to be sufficient reason to doubt the validity of self-reports of intention as operationalisations of intention in situations where reporting a high intention can serve as fear control. And in any case, even if self-reports of intention can be salvaged as operationalisations of intentions in fear appeal research, at best, they remain a proxy for the intervention target. The goal is not to keep people smoking but with different intentions: the goal is behavior change. The lack of evidence that fear manipulations can influence behavior on their own (even without strong self-efficacy manipulations) can mean either that this has not been studied, which would be a serious problem, or that these studies have not been published. Given the prevalence of publication bias, such a suppression of null findings would not be surprising. Whichever of these two scenarios turns out to be the case, given the current available evidence, there is little reason to expect behavior change from fear manipulations.

## Affect as mediator

Peters and Shoots-Reinhard (2018) then argue that our focus on behavior made us overlook that emotional reactions can influence judgmenets and decisions about smoking (also see [Mediators (determinants) targeted by warning labels]). We are not sure whether their departure from this focus on behavior implies that these emotional reactions are not eventually expected to impact behavior. If they are not, it is not clear how it can be justified to 'spend' the scarce resource of a health communication opportunity (e.g. on tobacco packaging) to elicit emotional reactions if the goal of such elicitation is not to help people cease smoking, or refrain from starting smoking. Therefore, we assume that these emotional reactions are expected to eventually impact behavior.

In that case, where emotional reactions are means to change behavior, we are not sure why a focus on behavior should be abandoned (or is even undesirable or unwise in any way). Showing effects on theorized mediators is of little use if it is not possible to show effects on the actual target variable.

However, for the sake of the discussion, let us follow this line of reasoning. They then cite Brewer et al. (2007) and Sheeran, Harris and Epton (2014) to support their statement that greater risk perceptions have been significantly associated with behaviors in meta-analyses. This is true: risk perception predicts behavior. However, compared to other determinants, it is a weak predictor (see [The importance of risk perception for behavior]). Succesfully changing risk perception is relatively unlikely to yield behavior change; if behavior change is the goal, it would be much wiser to target another (stronger) determinant of behavior.

It is interesting that in their support of knowledge as relevant determinant of behavior, they explain that:

> An understanding of available options is important in order to choose well between them [...]

We certainly agree on this: and would stress that *available* is the operative term here. In other words, telling people that behaving differently has benefits is not useful if they are not confident that behaving differently is an option available to them, for example, because a smoker has tried to quit before and failed (also see [Smokers’ low self-efficacy]).

Their sentence continues with:

> smokers’ knowledge of tobacco’s health risks is low, and most adults can only name between one and four of the many smoking-related diseases (Evans et al., 2015; Smith et al., 2010).

Here, the conviction is expressed that knowing and being able to list all relevant diseases is a requirement, or at least a strong facilitator, of the target behavior. However, in our view, this assumption is both inconsistent with common sense and, as far as we know, unsupported by evidence. It is inconsistent with common sense because it is unlikely that people who exercise regularly can name more diseases prevented by exercise than people who do not (after correction for education level); that people who consistently use condoms can name more sexually transmitted infections than people who do not; or that people who do not drink alcohol can name more diseases caused by alcohol consumption than people who do not. In other words: it does not seem plausible that simply being able to list many diseases is predictive of behavior. This, of course, could be explained by the poor predictive value of risk perception.

But, this argument could easily be countered by the observation that no evidence exists to support this statement; it remains purely theoretical. And in the absence of such evidence, subjective utility theories (e.g., holding that the more diseases one knows, the less likely one is to engage in a risk behavior) could be argued to be plausible models as well [^1]. Of course, in the absence of such evidence, one has to resort to Occams razor, and as a result, the simplest possible model, which is one where no association exists between number of diseases listed and the health behavior.

[^1]: Well, perhaps not subjective utility models specifically, given that such models assuming that humans are rational actors have been widely discredited by now.

However, we aim to conduct a study to provide this evidence. The Open Science Framework project page for this project is at [https://osf.io/mncu9/](https://osf.io/mncu9/). In this study, we will test whether knowledge of the details of diseases associated to a large number of health behaviors is associated to performing those behaviors.

Even if there turns out to be an associaton, though, it is unlikely to be strong, given that this type of knowledge would fall under the determinant risk perception, which we already know is a relatively weak predictor of behavior.

None of the studies cites in this section on functions of affect provides evidence that influence exerted by such emotional reactions can translate to behavior. This is odd; given that no study has successfully demonstrated transfer of effects on theorized mediators to behavior, to neglect including behavior meausures appears odd. It may be, of course, that behavior measures *are* regularly included, but that the null effects obtained on behavior, in combination with publication bias, prevent those findings from being published.

## Pictorial warnings as one step in a larger system

Finally, Peters and Shoots-Reinhard (2018) agree that without efficacy support, effects of threatening communication are lower. They then state that warnings are only one component of a larger-scale intervention. To which we say: why would you want a component that you already know has mediocre effectiveness at best, when that component can be replaced with another that can be more effective?

There seems to be some insistence on retaining warning labels. Developing behavior change interventions requires two steps that are both common sense and theoretically sensible: first, establish which determinants are the best intervention targets (i.e. are most strongly associated to behavior; see e.g. Peters, 2014), and second, establish which methods for behavior change are most likely to achieve large effects in those determinants (see Kok, 2014). Insistence on one specific method that targets one specific determinant (i.e. warning labels) seems odd. Especially because the defense of warning labels consists not of meta-analyses showing that yes, risk perception is the strongest predictor of successfully refraining from trying out tobacco or of successfully quitting smoking, and that yes, warning labels are the most potent available method to increase those risk perceptions, but instead of studies the results of which happen to not be inconsistent with warning labels having no effect whatsoever.

The fact that our suggestion is taken to be a call to explore new, innovative methods, may be indicative as to the underlying problem. This was a misunderstanding. We meant to suggest that application of time-honoured best practices to assistent smokers and vulnerable nonsmokers is long overdue. The basic logic of first establishing the relevant determinants and then consulting psychological theory to learn how to most effectively influence those determinants has been best practice for decades already (e.g. Bartholomew, Parcel & Kok, 1998). Perhaps this approach to prevention has not yet disseminated as widely as it deserves, which may help to explain the insistence on warning labels as health promotion intervention.

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# Roberto, Mongeau & Liu: A (Re)defining moment for fear appeals

Roberto, Mongeau and Liu (2018) observe that the term 'fear appeal' is often used incorrectly, and often interchangeably with *threatening communication*. They offer an interesting alternative definition:

> A *fear appeal* is a persuasive message designed to both create and alleviate of the emotion of fear in audience members.

This is a bit broader than the definition offered for 'fear arousal' in the taxonomy of behavior change methods by Kok et al. (2016; Table 3 in the supplementary materials at [https://osf.io/sqtuz/](https://osf.io/sqtuz/)):

> Arousing negative emotional reactions in order to promote self-protective motivation and action.

Accompanied by the following parameters for effectiveness:

> Requires high self-efficacy expectations rather than high outcome expectations alone.

The definition by Roberto, Mongeau and Liu includes 'fear alleviation' as a parameter for effectiveness, which at first glance appears broader than high self-efficacy (which, by enabling danger control, enables fear alleviation). Of the potential options, Roberto, Mongeau and Liu, 2018, rightly emphasize justified fear alleviation (i.e. adaptive behavior, which does alleviate the threat), and stress that a fear appeal should contain both threat and efficacy components.

They process to discuss the literature from the perspective of this definition, and on the basis of this thorough and enlightening, albeit somewhat disheartening, discussion, they suggest that the present confused state of the literature is caused by the variation in how fear appeals are applied and studied. Specifically, studies and literature syntheses rarely respect the need for all four components to be present. We agree that this is likely one of the causes, perhaps the most important one; and we think that if this debate were to contribute to improving this situation, it will already have been worthwhile.

Roberto, Mongeau and Liu (2018) continue to question our claim that:

> Fear appeals will in the majority of situations not change risk behavior in the desired direction, as for most behaviors targeted in health promotion interventions, self-efficacy is not high.

They list a number of behaviors for which self-efficacy may well be higher, and conclude that:

> fear appeals will not always be the most appropriate or effective behavioral-change strategy. For example, sometimes it may not be possible to create or alleviate fear.

We agree in so far that for some populations and behaviors, it is possible that risk perception (or another determinant targeted by fear appeals) is the most important determinant of behavior, and fear appeals are the most effective method of targeting this determinant.

However, we would argue that self-efficacy is only high in minority of situations. If a target population is rightly confident that they can easily change a behavior, they often will; therefore, in most of those situations, behavior change experts will not be involved as promotion of behavior change will be relatively simple.

In addition, the question is not whether fear appeals can work. Having some effect on behavior does not justify the use of fear appeals: the question is whether they are the most effective method. This, we believe, will rarely be the case.

Nonetheless, we took the admonition of Roberto, Mongeau and Liu (2018) to heart, and in this rejoinder, attempt to work towards consensus rather than adding to the confusion.

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# White & Albarracin: Investigating Belief Falsehood. Fear Appeals Do Change Behavior in Experimental Laboratory Studies

The commentary by White & Albarracin discusses the results of novel analyses that were previously unreported, and we share their delight at these results now being part of the published literature.

In their first analysis, they address exactly one of the problems in meta-analytic approaches that we have discussed in our original paper and elsewhere in this rejoinder: the validity of the operationalisation of the independent variable. White & Albarracin report the aggregated effect sizes found for behavior when threatening communications were compared to a passive control group. They find a 95% confidence interval of [0.207; 0.506], an effect that decreased but remained positive when the behavior was not a one-shot behavior (95% CI of [0.114; 0.452]).

As White & Albarracin (2018) conclude,

> when fear appeals are compared with no message or with a message with a lower fear level, the answer to the question of efficacy is a resounding “yes, they work!”

## A resounding "Yes, they work!"

We retain some reservations which we will explain below, but first let us assume for the sake of the argument that we agree: this would, however, lead us to reiterate the point partly made by White and Albarracin (2018) as well: 

> However, our team has also shown that fear is less effective than more sophisticated and expensive interventions such as those designed to train new behavioral skills among recipients, although they are rarely less effective than no intervention (c.f. Earl & Albarracín, 2007; for other emotional appeals, see Earl, Nisson, & Albarracín, 2015).

Fear is not the only or the best way to go. It is not even the cheapest way to go. One of the benefits of warning labels is that, once the required legislation is in place, the costs are almost entirely borne by the industry. However, this is not a characteristic of warning labels: it is a characteristic of health communications on packages of cigarettes. Such health communications can target any determinant of behavior, using any of the multitude of behavior change methods. Exploiting this advantage associated to information on tobacco packaging in no way restrains that information to fear inducing information.

Therefore, the question of efficacy so resoundingly answered by White and Albarracin (2018) with a "yes, they work!" is in fact a question that does not need to be answered in order to realise optimal behavior change potential. The question is not "do fear appeals work?", the question is "which health communication should be included on tobacco packaging the achieve the most effect?". As we probably all teach when we train students and professionals in systematic development of theory and evidence based behavior change interventions, the reply to that question is always at least two counter-questions. First, "what are the most important determinants of the target behavior", and second, "which behavior change methods are most effective to change these determinants?".

## A premature "Yes, they work!"

Our reservations are based on two points. The first is that eyeballing Figure 1 suggests to us that there may be publication bias; there appears to be a positive association between the effect on behavior and the variance of that effect size. The most well-known parameter determining effect size variance (the width of its sampling distribution) is sample size (more data points yield tighter sampling distributions with lower variance), but better designs and higher quality operationalisations can also decrease error variance and thereby decrease effect size variance. In general, better studies yield more accurate effect size estimates, and therefore those studies receive bigger weights in meta-regressions. If effect size variance is positively associated to effect size, that implies that studies that among the included studies, those studies finding the smallest effects are the highest-quality studies; a scenario implying that the studies finding the largest effects may represent, for example, Type 1 errors. Unfortunately, they did not fully disclose (Peters, Abraham & Crutzen, 2012) their data and analysis scripts. 

The second reservation concerns the risks inherent in meta-analyses to answer a research question that was not the research question of the included studies. The problem is that it is easy to overlook methodological and theoretical conditions that need to be met for data to be able to contribute to answering specific research questions. For example, in this specific example (also see Kok et al., 2018), a study to test the Extended Parallel Proces Model and its predicted additive and moderating effects requires a design where all four variables are manipulated (also see Roberto, Mongeau & Liu, 2018). Failing such a design, validity of the test of fear appeals is jeopardized. Conclusions from literature syntheses that disregard these dynamics suffer from the same diminished validity, and so urge considerable caution. At least enough caution to warrant resounding conclusions remature.

## Are truly fear-inducing appeals associated with positive behavior change? {.tabset}

### Text

It appears from White and Albarracin's(2018) Figure 2 that there is an inverse relationship between effect size variance (often reflecting sample size, but can also reflect operationalisation and design quality) and the effect on fear. Assuming the size of each circle reflects weight in the random effects meta-regression, the studies with the largest weight (and thus, the lowest effect size variance, which often means they had the largest sample size or highest methodological quality) achieve the smallest effect on fear. However, these high-quality studies achieve varying effects on behavior, whereas the lowest quality studies have both the highest effects on fear and on behavior.

The effect sizes found in low quality studies (e.g. underpowered studies) derive from wider sampling distributions, which means that the probability of finding large effects is larger, even when no effect exists in the population. At the same time, low quality studies often do not get published until they obtain large effects. For example, such large effects are often (erroneously) used to justify under-powered designs, as a priori power analyses for large effects yield small required sample sizes. Low quality (e.g. underpowered) studies are therefore particularly circumspect.

At the same time, these lowest quality studies appear mostly responsible for the observed association between fear and behavior change. In fact, when subjecting their plot to the `digitize` R package to extract each data points' fear and behavior effect size estmiates, and recomputing the regression without those two smallest studies, the regression coefficient is no longer distinguishable from zero.

### Analysis

```{r white-albarracin-digitize}

safeRequire('digitize');
whiteAlbarracinPlot <- here('white-albarracin-2.png');

if (interactive()) {
  digitize(whiteAlbarracinPlot);
}

### Digitizer returns:
#                x          y
# 1  -1.504425e-01 -0.2992701
# 2   9.734513e-01  0.1459854
# 3   1.044248e+00  0.5401460
# 4   1.548673e+00  0.6131387
# 5   1.176991e+00  0.8029197
# 6   1.212389e+00  0.8686131
# 7   2.220446e-16  0.9562044
# 8  -1.504425e-01  0.2189781
# 9   2.902655e+00  1.9781022
# 10  3.690265e+00  1.1532847

whiteAlbarracinDat <- as.data.frame(matrix(scan(text="
 -1.504425e-01 -0.2992701
  9.734513e-01  0.1459854
  1.044248e+00  0.5401460
  1.548673e+00  0.6131387
  1.176991e+00  0.8029197
  1.212389e+00  0.8686131
  2.220446e-16  0.9562044
 -1.504425e-01  0.2189781
  2.902655e+00  1.9781022
  3.690265e+00  1.1532847
"), byrow=TRUE, ncol=2));

names(whiteAlbarracinDat) <- c('d.fear', 'd.behavior');

whiteAlbarracinOriginalRegr <- 
  regr(d.behavior ~ d.fear,
       whiteAlbarracinDat,
       plot=TRUE);

whiteAlbarracinNewRegr <- 
  regr(d.behavior ~ d.fear,
       whiteAlbarracinDat[1:8, ],
       plot=TRUE);

caption1 <- paste0(figCountr(),
                   'Reproduction of the original plot by White & Albarracin (2018).');

caption2 <- paste0(figCountr(),
                   'Reproduction with the low-quality studies omitted.');

```

```{r white-albarracin-digitize-plot1, fig.cap=caption1}
print(whiteAlbarracinOriginalRegr);
```

```{r white-albarracin-digitize-plot2, fig.cap=caption2}
print(whiteAlbarracinNewRegr);
```

## Should policies continue to employ fear appeals because they appear to induce positive effects?

White and Albarracin (2018) conclude:

> [...] that fear appeals research should continue and that policies should continue to employ fear appeals because, at the population level, they appear to induce positive effects for a wide variety of behaviors.

As we argued before, showing positive effects is not enough to justify endorsement from health psychologists. Fear appeals are but one method targeting but one determinant (or in the best case scenario, a small number of determinants). Behavior has many other determinants, and many other methods exist to target those.

Having some positive effect, therefore, does not suffice to warrant widespread use. We should recommend people use the most effective method, to target the most important determinant. This means that we should recommend that determinant studies are conducted first, followed by inspection of the literature to select the best available methods for behavior change. As this entails considerably more work, it may not be a popular message, but it holds the promise of much stronger effects on behavior.

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# Towards the rejoinder

This debate has laid bare a number of fundamental disagreements. We have started a project to systematically generate the empirical evidence we need to resolve these disagreements (see [https://osf.io/a2v46/](https://osf.io/a2v46/)).

At the same time, already at present, there is also considerable consensus. All involved authors agree:

- that the health problems necessitating prevention efforts are serious;
- that threatening communication works best combined with a component that effectively increases perceived efficacy (i.e. response efficacy and self-efficacy); and
- that behavior is predicted by many determinants, of which risk perception, disgust, affect, and self-efficacy are some examples.

Based on this consensus, we wrote the main rejoinder text.

We hope that we made clear that we do not argue that fear appeals can never be effective. However, not having any discernible effect whatsoever does not in itself justify using a method. There are hundreds of available methods for behavior change, dozens of which lend themselves for application in health communications on, for example, tobacco packacing, targeting dozens of psychological determinants of behavior. Fear appeals are just one method, targeting just one determinant (risk perception). Amongst this fierce competition, not having zero effect doesn't cut it. The best method to use is the method that targets the most important determinant of behavior, and that has the strongest effect on that determinant.

The fact that this entire discussion revolves around 'whether or not fear appeals can have an effect' should in itself already be enough to justify preference for other methods, targeting other determinants of behavior, such as attitude, skills, self-efficacy, perceived norms, self-monitoring, action planning, and coping planning (the list does go on and on). This persistent focus on fear appeals, and the resulting implication that whether to use fear appeals depends on whether or not fear appeals *can have an effect*, prevents use of methods for behavior change that have a stronger effect, on determinants that are more strongly associated to behavior.

It is time to stop acting as if threatening communications represent the sum total of our behavior change toolbox, and start thinking anew about how the rare opportunities for intervention, such as the space on tobacco packaging, can be used to have the largest effect the ultimately eliminate smoking.

<!--------------------------------------------------------------------->
<!--------------------------------------------------------------------->

# References

This reference list includes the sources cited in this supplementary file as well as in the rejoinder as printed in Health Psychology Review.

<div style="margin-left: 2em; text-indent: -2em;">

Abraham, C., & Michie, S. (2008). A taxonomy of behavior change techniques used in interventions. *Health Psychology, 27*, 379–87. doi: [10.1037/0278-6133.27.3.379](https://doi.org/10.1037/0278-6133.27.3.379)

Ajzen, I. (2015). The theory of planned behaviour is alive and well, and not ready to retire: A commentary on Sniehotta, Presseau, and Araújo-Soares. *Health Psychology Review, 9*, 131-137. doi: [10.1080/17437199.2014.883474](https://doi.org/10.1080/17437199.2014.883474)

Bartholomew, L. K., Parcel, G. S., & Kok, G. (1998). Intervention Mapping: A process for developing theory- and evidence-based health education programs. *Health Education and Behavior, 25*, 545–563. doi: [10.1177/109019819802500502](https://doi.org/10.1177/109019819802500502)

Bartholomew Eldredge, L. K., Markham, C. M., Ruiter, R. A. C., Fernández, M. E., Kok, G., & Parcel, G. S. (2016). *Planning Health Promotion Programs: An Intervention Mapping Approach* (4th ed.). Hoboken, NJ, CA: Wiley.

Bartlett, Y. K., Sheeran, P., & Hawley, M. S. (2014). Effective behaviour change techniques in smoking cessation interventions for people with chronic obstructive pulmonary disease: A meta-analysis. *British Journal of Health Psychology, 19*, 181–203. doi: [10.1111/bjhp.12071](https://doi.org/10.1111/bjhp.12071)

Brewer, N.T., Hall, M.G. & Noar, S.M. (2018) Pictorial cigarette pack warnings increase quitting: A comment on Kok et al. *Health Psychology Review, 12*. doi: [10.1080/17437199.2018.1445544](https://doi.org/10.1080/17437199.2018.1445544)

Brewer, N. T., Hall, M. G., Noar, S. M., Parada, H., Stein-Seroussi, A., Bach, L. E., … Ribisl, K. M. (2016). Effect of Pictorial Cigarette Pack Warnings on Changes in Smoking Behavior. *JAMA Internal Medicine, 176*, 905. doi: [10.1001/jamainternmed.2016.2621](https://doi.org/10.1001/jamainternmed.2016.2621)

Borland, R. (2018) Misinterpreting theory and ignoring evidence: fear appeals can actually work: A comment on Kok et al. (2018). *Health Psychology Review, 12*. doi: [10.1080/17437199.2018.1445545](https://doi.org/10.1080/17437199.2018.1445545)

Buller, D., Borland, R., & Burgoon, M. (1998). Impact of behavioural intention on effectiveness of message features: Evidence from the family sun safety project. *Human Communication Research, 24*, 433-453. 

Chen, H., Cohen, P., & Chen, S. (2010). How big is a big odds ratio? Interpreting the magnitudes of odds ratios in epidemiological studies. *Communications in Statistics: Simulation and Computation, 39*, 860–864. doi: [10.1080/03610911003650383](https://doi.org/10.1080/03610911003650383)

Conner, M., & Norman, P. (2015). *Predicting and changing health behaviour: Research and practice with social cognition models* (3rd Ed.). London: Open University Press. ISBN-10: 033526378X

Crutzen, R., & Peters, G.-J. Y. (2018). Evolutionary learning processes as the foundation for behaviour change. *Health Psychology Review, 12*, 43-57. doi: [10.1080/17437199.2017.1362569](https://doi.org/10.1080/17437199.2017.1362569)

Crutzen, R., Peters, G-J. Y., & Noijen, J. (2017). Using confidence interval-based estimation of relevance to select social-cognitive determinants for behavior change interventions. *Frontiers in Public Health, 5*. doi: [10.3389/fpubh.2017.00165](https://doi.org/10.3389/fpubh.2017.00165)

Gelman, A., & Loken, E. (2014). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. *Psychological Bulletin, 140*, 1272–1280. doi: [10.1037/a0037714](https://doi.org/dx.doi.org/10.1037/a0037714)

Glanz, K., Rimer, B. K., & Viswanath, K., Eds. (2015). *Health behavior: Theory, research, and practice* (5th ed.). San Francisco, CA: Jossey-Bass. ISBN: 978-1-118-62898-0

Green, D. P., Ha, S. E., & Bullock, J. G. (2010). Enough Already about “Black Box” Experiments: Studying Mediation Is More Difficult than Most Scholars Suppose. *The ANNALS of the American Academy of Political and Social Science, 628*, 200–208. doi: [10.1177/0002716209351526](https://doi.org/10.1177/0002716209351526)

Kleinot, M. C., & Rogers, R. W. (1982). Identifying effective components of alcohol misuse prevention programs. *Journal of Studies on Alcohol, 43*, 802–811.

Kok, G. (2014). [A practical guide to effective behavior change: How to apply theory- and evidence-based behavior change methods in an intervention.](https://ehps.net/ehp/index.php/contents/article/view/ehp.v16.i5.p156/8) *European Health Psychologist, 16*, 156–170.

Kok, G., Gottlieb, N. H., Peters, G. Y., Mullen, P. D., Parcel, G. S., Ruiter, R. A. C., … Bartholomew, L. K. (2016). A taxonomy of behavior change methods: an Intervention Mapping approach. *Health Psychology Review, 10*, 297–312. doi: [10.1080/17437199.2015.1077155](https://doi.org/10.1080/17437199.2015.1077155)

Luszczynska, A. & Schwarzer, R.K., (2015). Health action process approach. In: M. Conner, & P. Norman, *Predicting and changing health behaviour: Research and practice with social cognition models* (3rd Ed.). London: Open University Press. ISBN-10: 033526378X

Malouff, J. (2018) What constitutes evidence that fear appeals have positive effects on health behavior? Commentary on Kok, Peters, Kessels, ten Hoor, and Ruiter (2018). *Health Psychology Review, 12*. doi: [10.1080/17437199.2018.1445541](https://doi.org/10.1080/17437199.2018.1445541)

Masters, R., Anwar, E., Collins, B., Cookson, R., & Capewell, S. (2017). Return on investment of public health interventions: a systematic review. *Journal of Epidemiology & Community Health, 71*, 827–834. doi: [10.1136/jech-2016-208141](https://doi.org/10.1136/jech-2016-208141)

McEachan, R. R. C., Conner, M., Taylor, N. J., & Lawton, R. J. (2011). Prospective prediction of health-related behaviours with the Theory of Planned Behaviour: a meta-analysis. *Health Psychology Review, 5*, 97–144. doi: [10.1080/17437199.2010.521684](https://doi.org/10.1080/17437199.2010.521684)

Michie, S., Richardson, M., Johnston, M., Abraham, C., Francis, J., Hardeman, W., … Wood, C. E. (2013). The Behavior Change Technique Taxonomy (v1) of 93 Hierarchically Clustered Techniques: Building an International Consensus for the Reporting of Behavior Change Interventions. *Annals of Behavioral Medicine : A Publication of the Society of Behavioral Medicine.* doi: [10.1007/s12160-013-9486-6](https://doi.org/10.1007/s12160-013-9486-6)

Niederdeppe, J. & Kemp, D (2018) Ignoring theory and evidence: Commentary on Kok et al. (2018), *Health Psychology Review, 12*. doi: [10.1080/17437199.2018.1445543](https://doi.org/10.1080/17437199.2018.1445543)

Ntoumanis, N., Healy, L. C., Sedikides, C., Smith, A. L., & Duda, J. L. (2014). Self-Regulatory Responses to Unattainable Goals: The Role of Goal Motives. *Self and Identity, 13*, 494–612. doi: [10.1080/15298868.2014.889033](https://doi.org/10.1080/15298868.2014.889033)

Ntoumanis, N., & Sedikides, C. (2018). Holding on (to the goal) or letting it go and move on? A tripartite model of goal striving. *Current Directions in Psychological Science*. Retrieved from https://www.researchgate.net/publication/323676131_Holding_on_to_the_goal_or_letting_it_go_and_move_on_A_tripartite_model_of_goal_striving

Ogden, J. (2016). Celebrating variability and a call to limit systematisation: the example of the Behaviour Change Technique Taxonomy and the Behaviour Change Wheel. *Health Psychology Review, 10*, 245–250. doi: [https://doi.org/10.1080/17437199.2016.1190291](https://doi.org/https://doi.org/10.1080/17437199.2016.1190291)

Peters, G.-J. Y. (2014). [A practical guide to effective behavior change: how to identify what to change in the first place.](https://ehps.net/ehp/index.php/contents/article/view/ehp.v16.i5.p142) *European Health Psychologist, 16*, 142–155.

Peters, G.-J. Y., Abraham, C., & Crutzen, R. (2012). [Full disclosure: doing behavioural science necessitates sharing.](https://ehps.net/ehp/index.php/contents/article/view/777/ehp.v14.i4.p77) *The European Health Psychologist, 14*, 77–84.

Peters, G.-J. Y., de Bruin, M., & Crutzen, R. (2015). Everything should be as simple as possible, but no simpler: towards a protocol for accumulating evidence regarding the active content of health behaviour change interventions. *Health Psychology Review, 9*, 1–14. doi: [10.1080/17437199.2013.848409](https://doi.org/10.1080/17437199.2013.848409)

Peters, G.-J. Y., & Crutzen, R. (2017). Pragmatic Nihilism: How a Theory of Nothing can help health psychology progress. *Health Psychology Review, 11*, 103–121. doi: [10.1080/17437199.2017.1284015](https://doi.org/10.1080/17437199.2017.1284015)

Peters, G.-J. Y., & Kok, G. (2016). All models are wrong, but some are useful: A comment on Ogden (2016). *Health Psychology Review, 10*(3)*, 265–268. doi: [10.1080/17437199.2016.1190658](https://doi.org/10.1080/17437199.2016.1190658)

Peters, G-J.Y. Ruiter, R.A.C. & Kok G. (2014). Threatening communication: A qualitative study of fear appeal effectiveness beliefs among intervention developers, policy makers, politicians, scientists, and advertising professionals. *International Journal of Psychology, 49* (2)*, 71-79. doi: [10.1002/ijop.12000  ](https://doi.org/10.1002/ijop.12000)

Peters, G.-J.Y., Ruiter, R. A. C., Verboon, P., & Kok, G. (2017, April 30). Threatening communication: Diffusing the scientific evidence on fear appeal effectiveness among intervention developers and other groups of key actors. *PsyArXiv*, retrieved from https://psyarxiv.com/vehtx

Peters, E. & Shoots-Reinhard, B. (2018) Don’t throw the baby out with the bath water: Commentary on Kok, Peters, Kessels, ten Hoor, and Ruiter (2018). *Health Psychology Review, 12*. doi: [10.1080/17437199.2018.1445542](https://doi.org/10.1080/17437199.2018.1445542)

Roberto, A.J., Mongeau, P.A. & Liu, Y.(2018) A (re)defining moment for fear appeals: A comment on Kok et al. (2018). *Health Psychology Review, 12*. doi: [10.1080/17437199.2018.1445546](https://doi.org/10.1080/17437199.2018.1445546)

Rogers, R. W., & Mewborn, C. R. (1976). Fear appeals and attitude change: effects of a threat’s noxiousness, probability of occurrence, and the efficacy of coping responses. *Journal of Personality and Social Psychology, 34*, 54–61.

Roe, R. A. (2012). [What is wrong with mediators and moderators?](https://ehps.net/ehp/index.php/contents/article/view/ehp.v14.i1.p4/31) *The European Health Psychologist, 14*, 4–10.

Rosenthal, J. A. (1996). Qualitative descriptors of strength of association and effect size. *Journal of Social Service Research, 21*, 37–59. doi: [10.1300/J079v21n04_02](https://doi.org/10.1300/J079v21n04_02)

Ruiter, R. A. C., Abraham, C., & Kok, G. (2001). Scary warnings and rational precautions: A review of the psychology of fear appeals. *Psychology & Health, 16*, 613–630. doi: [10.1080/08870440108405863](https://doi.org/10.1080/08870440108405863)

Sheeran, P., & Webb, T. L. (2016). The Intention–Behavior Gap. *Social and Personality Psychology Compass, 10*(9)*, 503–518. doi: [10.1111/spc3.12265](https://doi.org/10.1111/spc3.12265)

Shtulman, A., & Valcarcel, J. (2012). Scientific knowledge suppresses but does not supplant earlier intuitions. *Cognition, 124*, 209-215. https://dx.doi.org/10.1016/j.cognition.2012.04.005

Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant. *Psychological Science, 22*, 1359–66. doi: [10.1177/0956797611417632](https://doi.org/10.1177/0956797611417632)

Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2012). A 21 Word Solution. *SSRN Electronic Journal*, 1–4. doi: [10.2139/ssrn.2160588](https://doi.org/10.2139/ssrn.2160588)

Spencer, S. J., Zanna, M. P., & Fong, G. T. (2005). Establishing a causal chain: Why experiments are often more effective than mediational analyses in examining psychological processes. *Journal of Personality and Social Psychology, 89*, 845–851. doi: [10.1037/0022-3514.89.6.845](https://doi.org/10.1037/0022-3514.89.6.845)

ten Hoor, G.A., Peters, G-J.Y., Kalagi, J. de Groot, L., Grootjans, K., Huschens, A., Kohninger, C., Kolgen, L., Pelssers, I., Schutt, T., Thomas, S., Ruiter R.A.C. & Kok, G., 2012. Reactions to threatening health messages. *BMC Public Health, 12*. doi: [10.1186/1471-2458-12-1011](https://doi.org/10.1186/1471-2458-12-1011)

Thrasher, J. F., Osman, A., Abad-Vivero, E. N., Hammond, D., Bansal-Travers, M., Michael Cummings, K., Hardin, James W. & Moodie, C. (2015). The use of cigarette package inserts to supplement pictorial health warnings: An evaluation of the canadian policy. *Nicotine and Tobacco Research, 17*, 870–875. doi: [10.1093/ntr/ntu246](http://doi.org/10.1093/ntr/ntu246)

Riet, J. van ‘t, & Ruiter, R. A. C. (2013). Defensive reactions to health-promoting information: an overview and implications for future research. *Health Psychology Review, 7*, S104–S136. doi: [10.1080/17437199.2011.606782](https://doi.org/10.1080/17437199.2011.606782)

Webb, T. L., & Sheeran, P. (2006). Does changing behavioral intentions engender behavior change? A meta-analysis of the experimental evidence. *Psychological Bulletin, 132*, 249–268. doi: [10.1037/0033-2909.132.2.249](https://doi.org/10.1037/0033-2909.132.2.249)

White, B. & Albarracín, D. (2018) Investigating belief falsehood. Fear appeals do change behavior in experimental laboratory studies. A commentary on Kok et al. (2018). *Health Psychology Review, 12*,  doi: [10.1080/17437199.2018.1448292](https://doi.org/10.1080/17437199.2018.1448292)

Whittingham, J. R. D., Ruiter, R. A. C., Bolier, L., Lemmers, L., Hasselt, N. V., & Kok, G. (2009). Avoiding Counterproductive Results: An Experimental Pretest of a Harm Reduction Intervention on Attitude Toward Party Drugs Among Users and Nonusers. *Substance Use & Misuse, 44*, 532–547. doi: [10.1080/10826080802347685](https://doi.org/10.1080/10826080802347685)

Wilson, T. D., & Bar-Anan, Y. (2008). Psychology: The unseen mind. *Science, 321*, 1046–1047. doi: [10.1126/science.1163029](https://doi.org/10.1126/science.1163029)

Wilson, T. D., & Dunn, E. W. (2004). Self-Knowledge: Its Limits, Value, and Potential for Improvement. *Annual Review of Psychology, 55*, 493–518. doi: [10.1146/annurev.psych.55.090902.141954](https://doi.org/10.1146/annurev.psych.55.090902.141954)

</div>

```{r footnote-heading, results='asis', echo=FALSE}
  if (knitr::opts_knit$get("rmarkdown.pandoc.to") != "latex") {
    cat("\n\n# Footnotes\n\n");
  }
```
